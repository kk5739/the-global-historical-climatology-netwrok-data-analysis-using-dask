{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d87882-6ab1-4e45-9f56-97f94b00b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GHCN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ghcn import load_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LOCAL to True for single-machine execution while developing\n",
    "# Set LOCAL to False for cluster execution\n",
    "LOCAL = True\n",
    "\n",
    "if LOCAL:\n",
    "    # This line creates a single-machine dask client\n",
    "    client = Client()\n",
    "else:    \n",
    "    # This line creates a SLURM cluster dask and dask client\n",
    "    # Logging outputs will be stored in /scratch/{your-netid}\n",
    "    \n",
    "    cluster = SLURMCluster(\n",
    "                           # Memory and core limits should be sufficient here\n",
    "                           memory='4GB', cores=2,\n",
    "\n",
    "                           # Ensure that Dask uses the correct version of Python on the cluster\n",
    "                           python='/scratch/work/public/dask/{}/bin/python'.format(dask.__version__),                           \n",
    "                           \n",
    "                           # Place the output logs in an accessible location\n",
    "                           job_extra=['--output=/scratch/{}/slurm-%j.out'.format(os.environ['SLURM_JOB_USER'])]\n",
    "    )\n",
    "\n",
    "    cluster.submit_command = 'slurm'\n",
    "    cluster.scale(50)\n",
    "\n",
    "    display(cluster)\n",
    "    client = Client(cluster)\n",
    "\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all input files\n",
    "# We'll sort them alphabetically to ensure reproducibility\n",
    "\n",
    "#files = sorted(glob('/scratch/work/courses/DSGA1004-2021/ghcnd_tiny/*.dly'))\n",
    "#files = sorted(glob('/scratch/work/courses/DSGA1004-2021/ghcnd_small/*.dly'))\n",
    "files = sorted(glob('/scratch/work/courses/DSGA1004-2021/ghcnd_all/*.dly'))\n",
    "\n",
    "# Load in a single file to demonstrate the parser\n",
    "# Just print out the first few records to illustrate the structure\n",
    "load_daily(files[0])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load daily files into bags\n",
    "\n",
    "# Create a Dask Bag from the files\n",
    "bag = db.from_sequence(files).map(load_daily).flatten()\n",
    "\n",
    "# Filter for valid TMAX and TMIN observations\n",
    "bag = bag.filter(lambda x: x['value'] != -9999 and x['quality'] == ' ' and x['element'] in ('TMAX', 'TMIN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475e29a-5650-4fba-b3df-118b5fee3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregated statistics\n",
    "\n",
    "# Simplify each dictionary for DataFrame conversion\n",
    "def extract_core_fields(d):\n",
    "    return {\n",
    "        'station_id': d['station_id'],\n",
    "        'year': d['year'],\n",
    "        'month': d['month'],\n",
    "        'day': d['day'],\n",
    "        'element': d['element'],\n",
    "        'value': d['value'],\n",
    "    }\n",
    "\n",
    "# Transform bag → DataFrame\n",
    "bag_reduced = bag.map(extract_core_fields)\n",
    "df = bag_reduced.to_dataframe(meta={\n",
    "    'station_id': str,\n",
    "    'year': int,\n",
    "    'month': int,\n",
    "    'day': int,\n",
    "    'element': str,\n",
    "    'value': int\n",
    "})\n",
    "\n",
    "# Split TMAX and TMIN\n",
    "df_tmax = df[df['element'] == 'TMAX'].rename(columns={'value': 'TMAX'}).drop(columns='element')\n",
    "df_tmin = df[df['element'] == 'TMIN'].rename(columns={'value': 'TMIN'}).drop(columns='element')\n",
    "\n",
    "# Join on date\n",
    "df_merged = dd.merge(df_tmax, df_tmin, on=['station_id', 'year', 'month', 'day'], how='inner')\n",
    "\n",
    "# Compute t_range\n",
    "df_merged['t_range'] = df_merged['TMAX'] - df_merged['TMIN']\n",
    "\n",
    "# Get the max t_range per station\n",
    "result = df_merged.groupby('station_id')[['t_range']].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7dad1-d5e4-40ff-9138-6cef2c380139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "#!ls -lh tdiff.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcd108-dbc2-4673-aa87-3485e32113f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving issue in Dask; 1st try & approach\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "# choose which size you’re writing\n",
    "# 'tiny', 'small', 'all'\n",
    "size = 'all' \n",
    "out = f'tdiff-{size}.parquet'\n",
    "\n",
    "# Cast station_id to string everywhere so all partitions match\n",
    "result = result.assign(station_id=result.station_id.astype(str))\n",
    "\n",
    "# Persist the already-computed `result` DataFrame in memory\n",
    "result = result.persist()\n",
    "wait(result)  # block until it’s fully in RAM\n",
    "\n",
    "# Repartition to control chunk size / number of output files \"Optional\"\n",
    "# Tune n partitions based on your worker RAM (e.g. 2 for \"tiny\", 5 for \"small\", 10 for “all”)\n",
    "result = result.repartition(npartitions=7)\n",
    "\n",
    "# Time the parquet write with Dask’s own writer\n",
    "%time result.to_parquet(out, engine='pyarrow', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696e9d4-5ed6-423f-972b-850e73d50372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-stage reduction; 3rd try & approach - no use\n",
    "# Pre-partition max (no heavy shuffle)\n",
    "#def max_by_station(pdf):\n",
    "#    return pdf.groupby('station_id', as_index=False)['t_range'].max()\n",
    "\n",
    "#partial = df_merged.map_partitions(max_by_station, meta={'station_id': str, 't_range': df_merged['t_range'].dtype})\n",
    "\n",
    "# Final global max (small shuffle)\n",
    "#result = (partial.groupby('station_id')['t_range'].max().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b222-ead6-4021-9836-7c08d4a154a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving issue using pandas-based write; 2nd try & approach - no use \n",
    "\n",
    "# choose which size you’re writing: 'tiny', 'small', or 'all'\n",
    "#size = 'tiny'\n",
    "#out  = f'tdiff-{size}.parquet'\n",
    "\n",
    "# Compute into a single pandas DataFrame on the driver\n",
    "#df_pd = result.compute()\n",
    "\n",
    "# Time the pandas-based parquet write\n",
    "#%time df_pd.to_parquet(out, engine='pyarrow', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f75a05-0a30-4f7e-a9bb-8b439c12cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the saved Parquet file\n",
    "df_plot = dd.read_parquet(f'tdiff-{size}.parquet').compute()\n",
    "\n",
    "# Convert tenths of °C → °C\n",
    "df_plot['t_range_C'] = df_plot['t_range'] / 10\n",
    "\n",
    "# Sort and get top 10 stations\n",
    "Top_N = 10\n",
    "df_top = df_plot.sort_values('t_range_C', ascending=False).head(Top_N)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(df_plot['station_id'], df_plot['t_range_C'])\n",
    "\n",
    "# Add value labels just above each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,   # center X\n",
    "        height + 0.5,                        # slightly above top\n",
    "        f'{height:.1f}',                     # formatted label\n",
    "        ha='center', va='bottom', fontsize=9\n",
    "    )\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Max Daily Temp Range (°C x10)')\n",
    "plt.title('Top 10 Stations by Max Daily Temp Range')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d09373-e4a2-4415-9c67-7863b6d48565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
